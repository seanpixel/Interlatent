{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Ablations: End-to-end Notebook\n",
    "\n",
    "This notebook runs the full character-ablation workflow in one place:\n",
    "- Load the prompt dataset (character rewrites of `daily_dilemmas`).\n",
    "- Ask the model for single-line decisions for each prompt.\n",
    "- Visualize SAE latents per character.\n",
    "- List top tokens activating each latent.\n",
    "\n",
    "Assumptions:\n",
    "- Prompt CSV: `data/character_dilemmas.csv`\n",
    "- SAE checkpoint: `artifacts/sae_llm_layer_30_20251217_070930.pth`\n",
    "- Latent DB (from run_experiment): `latents_character_dilemmas.db`\n",
    "- Model: `mistralai/Ministral-3-14B-Instruct-2512`\n",
    "\n",
    "Set `RUN_MINISTRAL3=1` and `HF_TRUST_REMOTE_CODE=1` in the environment before running generation cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb24e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def resolve_path(default: Path, pattern: str):\n",
    "    if default.exists():\n",
    "        return default\n",
    "    matches = list(Path(\".\").rglob(pattern))\n",
    "    if matches:\n",
    "        print(f\"[info] Using discovered file: {matches[0]}\")\n",
    "        return matches[0]\n",
    "    raise FileNotFoundError(f\"Could not find {pattern}; tried default {default}\")\n",
    "\n",
    "# Paths / config (can override via env vars)\n",
    "DATA_CSV = resolve_path(Path(os.getenv(\"CHARACTER_CSV\", \"data/character_dilemmas.csv\")), \"character_dilemmas.csv\")\n",
    "SAE_PATH = resolve_path(Path(os.getenv(\"SAE_PATH\", \"artifacts/sae_llm_layer_30_20251217_070930.pth\")), \"sae_llm_layer_30_*.pth\")\n",
    "DB_PATH = resolve_path(Path(os.getenv(\"LATENT_DB\", \"latents_character_dilemmas.db\")), \"latents_character_dilemmas.db\")\n",
    "MODEL_ID = \"mistralai/Ministral-3-14B-Instruct-2512\"\n",
    "TARGET_LAYER = \"llm.layer.30\"\n",
    "MAX_NEW_TOKENS = 64  # cap for decision outputs; adjust if needed\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print(df.head())\n",
    "print(f\"Loaded {len(df)} prompts from {DATA_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str, trust_remote_code: bool):\n",
    "    from transformers import AutoConfig, Mistral3ForConditionalGeneration, MistralCommonBackend\n",
    "\n",
    "    tok = MistralCommonBackend.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    if tok.pad_token_id is None and getattr(tok, \"eos_token\", None):\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    config = AutoConfig.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    llm = Mistral3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "        config=config,\n",
    "        torch_dtype=dtype,\n",
    "        device_map={\"\": device},\n",
    "    )\n",
    "    llm.eval()\n",
    "    return tok, llm, device\n",
    "\n",
    "\n",
    "def decision_prompt(text: str) -> str:\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        \"Respond with a single-line final decision only. No explanations.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_decisions(prompts, tok, llm, *, max_new_tokens: int, device: str):\n",
    "    outputs = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        enc = tok([prompt], return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attn_mask = enc.get(\"attention_mask\")\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = llm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attn_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        text = tok.decode(out[0].tolist(), skip_special_tokens=True).strip()\n",
    "        outputs.append(text)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate one-line decisions for all prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"RUN_MINISTRAL3\") != \"1\":\n",
    "    raise RuntimeError(\"Set RUN_MINISTRAL3=1 and HF_TRUST_REMOTE_CODE=1 before running this cell.\")\n",
    "\n",
    "trust_remote_code = os.environ.get(\"HF_TRUST_REMOTE_CODE\", \"1\") == \"1\"\n",
    "tok, llm, device = load_model_and_tokenizer(MODEL_ID, trust_remote_code)\n",
    "print(f\"Model on {device}\")\n",
    "\n",
    "prompts = [decision_prompt(t) for t in df[\"text\"].tolist()]\n",
    "decisions = generate_decisions(prompts, tok, llm, max_new_tokens=MAX_NEW_TOKENS, device=device)\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"decision\"] = decisions\n",
    "out_path = Path(\"vis/character_decisions.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(out_path, index=False)\n",
    "\n",
    "unique_decisions = sorted(set(decisions))\n",
    "print(f\"Decisions saved to {out_path}\")\n",
    "print(f\"Unique decisions: {len(unique_decisions)} / {len(decisions)}\")\n",
    "print(\"Sample decisions (first 5):\")\n",
    "for line in decisions[:5]:\n",
    "    print(\"  \", line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize SAE latents (image + CLI)\n",
    "Uses the existing helper functions from `visualize_latents.py` on the latent_sae layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.demos.ministral.character_ablations.visualize_latents import (\n",
    "    load_events,\n",
    "    aggregate,\n",
    "    plot_heatmap,\n",
    "    report_variation,\n",
    ")\n",
    "\n",
    "LATENT_LAYER = f\"latent_sae:{TARGET_LAYER}\"\n",
    "events = load_events(DB_PATH, LATENT_LAYER)\n",
    "mat, stds, labels = aggregate(events)\n",
    "out_png = Path(\"vis/character_latents.png\")\n",
    "plot_heatmap(mat, labels, out_png)\n",
    "print(f\"Saved latent heatmap to {out_png}\")\n",
    "\n",
    "lines = report_variation(mat, labels, stds, top_k=10)\n",
    "print(\"Top varying channels:\")\n",
    "for line in lines:\n",
    "    print(\"  \" + line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top tokens per latent (CLI)\n",
    "Uses the helper from `top_tokens.py` to print the most activating tokens per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.demos.ministral.character_ablations.top_tokens import load_events as load_events_tt, aggregate_token_stats\n",
    "\n",
    "events_tt = load_events_tt(DB_PATH, LATENT_LAYER, limit=None)\n",
    "stats = aggregate_token_stats(events_tt, min_count=3)\n",
    "topk = 10\n",
    "for ch in sorted(stats.keys()):\n",
    "    tokens = sorted(stats[ch].items(), key=lambda kv: kv[1][0], reverse=True)[:topk]\n",
    "    if not tokens:\n",
    "        continue\n",
    "    print(f\"channel {ch}:\")\n",
    "    for token, (mean_val, count, max_val) in tokens:\n",
    "        print(f\"  {token!r:12s} mean={mean_val: .4f} count={count:3d} max={max_val: .4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
